{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install requests\n",
    "# pip install beautifulsoup4\n",
    "# pip install nltk \n",
    "# pip install spacy\n",
    "# pip install textblob\n",
    "# pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL des Wahlprogramms abrufen\n",
    "\n",
    "url = 'https://www.fdp.de/das-wahlprogramm-der-freien-demokraten-zur-bundestagswahl-2025'\n",
    "\n",
    "response  = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "     page = response.text\n",
    "else:\n",
    "    print(f\"Seite konnte nicht abgerufen werden - Status Code {response.status_code}\")\n",
    "\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "del page, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gesamtanzahl der Wörter: 19237\n"
     ]
    }
   ],
   "source": [
    "# Liste zum Speichern der Inhalte in der richtigen Reihenfolge\n",
    "content_list = []\n",
    "\n",
    "# Erstes <h2>-Element suchen, um den Startpunkt zu setzen\n",
    "start_element = soup.find(\"h2\", class_=\"subheader\")\n",
    "\n",
    "# Falls ein <h2> gefunden wurde, ab diesem Element iterieren\n",
    "if start_element:\n",
    "    # Erstes h2-Element explizit hinzufügen\n",
    "    content_list.append(f\"### {start_element.get_text(strip=True)}\")\n",
    "\n",
    "    under_h3 = False  # Variable, um zu verfolgen, ob wir uns unter einer h3-Überschrift befinden\n",
    "    last_h2_index = None  # Index des letzten gespeicherten h2\n",
    "    downgrade_h2 = False  # Ob das letzte h2 zu ## umgewandelt werden muss\n",
    "\n",
    "    # Alle nachfolgenden relevanten Elemente sammeln\n",
    "    for element in start_element.find_all_next([\"h2\", \"h3\", \"label\", \"p\"]):\n",
    "        # Überschriften mit ### markieren\n",
    "        if element.name == \"h2\" and \"subheader\" in element.get(\"class\", []):\n",
    "            text = element.get_text(strip=True)\n",
    "            content_list.append(f\"### {text}\")\n",
    "            last_h2_index = len(content_list) - 1  # Speichert den Index des letzten h2\n",
    "            downgrade_h2 = False  # Standardmäßig bleibt es ###\n",
    "\n",
    "        # h3-Überschriften erkennen\n",
    "        elif element.name == \"h3\":\n",
    "            under_h3 = True  # Ab jetzt sind wir unter einer h3-Überschrift\n",
    "\n",
    "        # Thesen mit ## markieren, aber \"Unsere Mission\" ignorieren\n",
    "        elif element.name == \"label\" and \"accordion-label\" in element.get(\"class\", []):\n",
    "            text = element.get_text(strip=True)\n",
    "            if text == \"Unsere Mission\":\n",
    "                downgrade_h2 = True  # Falls vorher ein h2 war, soll es zu ## geändert werden\n",
    "            else:\n",
    "                content_list.append(f\"## {text}\")\n",
    "                under_h3 = False  # Falls eine These kommt, sind wir nicht mehr unter einer h3\n",
    "\n",
    "        # Falls das vorherige h2 downgegradet werden soll\n",
    "        if downgrade_h2 and last_h2_index is not None:\n",
    "            content_list[last_h2_index] = content_list[last_h2_index].replace(\"###\", \"##\")\n",
    "            downgrade_h2 = False  # Rücksetzen, damit es nur einmal passiert\n",
    "\n",
    "        # Alle Texte extrahieren, wenn sie NICHT unter einer h3 stehen\n",
    "        elif element.name == \"p\" and not under_h3:\n",
    "            text = element.get_text(\" \", strip=True)  # Leerzeichen für bessere Lesbarkeit\n",
    "            content_list.append(f\"# {text}\")\n",
    "\n",
    "# Entferne komplett leere Einträge oder Einträge mit nur Leerzeichen\n",
    "content_list = [entry.strip() for entry in content_list if entry.strip()]\n",
    "\n",
    "# Die extrahierten Inhalte speichern\n",
    "fdp_gesamt = \"\\n\".join(content_list)\n",
    "with open(\"fdp.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(fdp_gesamt)\n",
    "\n",
    "# Gesamtwortanzahl berechnen\n",
    "word_count = len(fdp_gesamt.split())\n",
    "print(f\"\\nGesamtanzahl der Wörter: {word_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
